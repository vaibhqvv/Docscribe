```markdown
# Docscribe – AI-Powered PDF Chatbot using RAG

Docscribe is a PDF-based chatbot built using Retrieval-Augmented Generation (RAG). It allows users to upload PDF files and ask natural language questions. The answers are generated by a Large Language Model (LLM), strictly grounded in the content of the uploaded documents.

This project combines the power of **LangChain**, **Groq LLMs (LLaMA3-70B)**, **HuggingFace Embeddings**, and **ChromaDB** into a modular, fast, and easy-to-use GenAI application.

---

## Features

- Upload multiple PDFs
- Ask natural language questions
- Answers based **only** on uploaded documents
- No hallucinations — fallback message when answer isn't found
- Fast response times using Groq LLaMA3-70B
- Clean, Streamlit-based web interface

## Tech Stack

| Layer       | Tech Used                             |
| ----------- | ------------------------------------- |
| Backend     | FastAPI, LangChain, ChromaDB, Groq    |
| Embeddings  | HuggingFace (`all-MiniLM-L12-v2`)     |
| PDF Parsing | PyPDFLoader (via langchain_community) |
| Frontend    | Streamlit                             |
| Logging     | loguru                                |

---

## Project Structure
```

Docscribe/
├── server/
│ ├── main.py # FastAPI entry point
│ ├── modules/
│ │ ├── llm.py # LLM prompt and RetrievalQA chain
│ │ ├── load_vectorstore.py # PDF loading, embedding, vectorstore
│ │ ├── pdf_handler.py # File save utility
│ │ ├── query_handler.py # Chain runner + logger
│ └── logger.py # Loguru setup
│ └── requirements.txt # Server dependencies
│
├── client/
│ ├── app.py # Streamlit UI entry point
│ ├── utils/api.py # Frontend API connectors
│ └── components/ # Upload, chat, and history UI modules
│
├── uploaded_pdfs/ # Stored uploaded PDFs
├── chroma_store/ # Persistent vectorstore

````

---

## Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/docscribe.git
cd docscribe
````

### 2. Set Up Environment Variables

Create a `.env` file in the `server/` folder and add:

```env
GROQ_API_KEY=your_groq_api_key
```

### 3. Install Dependencies

#### Server

```bash
cd server
pip install -r requirements.txt
uvicorn main:app --reload
```

#### Client

```bash
cd ../client
streamlit run app.py
```

---

## Test Endpoint

Once the backend is running:

- `http://localhost:8000/test` → Should return:

  ```json
  { "message": "Testing successful..." }
  ```

---

```

```
